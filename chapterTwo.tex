\chapter[Models]{Methods}

These are the methods that were used.  No single method is ideal, and using many methods is better for insight, especially for nonlinear/complex systems.
\note Expand

\section{Linear}

\subsection{Overview}
Linear modeling is the most basic form of model, where an output is directly related to an input (or inputs) by a constant coefficient. In the simplest form, it appears as $y=ax+b$ where $y$ is an output, $x$ is an input, $a$ is a constant coefficient relating the input to the output, and $b$ is a constant shifting factor to account for variables with a non-zero mean. \inote{cite?} Due to its simplicity and ease of application, it is used in practically all fields as a first attempt to discover information about the data and any potential relationships. 

\subsection{Correlation}
Correlation is the degree to which two variables are statistically similar. Mathematically, correlation is defined as: $corr(x,y)=\frac{E[(x-\mu_x)(y-\mu_y)]}{\sigma_x\sigma_y}$,\inote{Important enough to make full width equation?} which accounts for differences in variables caused by variance ($\sigma$) or mean ($\mu$). This allows an analysis of the underlying relationship between multiple measurements when they may not be measuring the same thing. For example, total rainfall will correlate with the water depth of a reservoir, despite being different kinds of measurements and the (somewhat) one-way causality of the relationship. 
\note Need better example. Also, "variable" vs "measurement"

\subsection{Impulse Response}

Impulse response systems are systems in which the output (a response) is driven by a linear sum of coefficients of an input (a series of impulses). A simple example would be making a loud noise in a concert hall. The response will be the unique echoes and reverberations created by the initial driving sound, and with enough noise, a statistical model can be generated that will map the input sound to a response echo. In the magnetosphere, the most used example is an impulse of $v_{B_s}$ driving the Auroral Electrojet (AE/AL) index \cite{VBzAL}, or the Disturbance Storm Time ($D_{st}$) \inote{How often is it worth restating the meaning of acronyms if they haven't been mentioned for a while?} index \cite{VBzDST}, also shown in Figure \ref{VBzIRplot}.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.40]{{Figures/VBzIR.png}}
\caption{DST (black), nonlinear autoregressive exogenous (ARX) model (red), Burton et al 1975 model (green). (b) $v_{B_S}$ impulse as input.\cite{ARXEqn}}
\label{VBzIRplot}
\end{figure}

\note Convert figures to eps

This plot shows how different models are used to predict magnetospheric variables with varying amounts of success. In this proposal, what starts as a simple Box-Jenkins model of the form \cite{DOYvar}:
\begin{align*}
x(t)&=c+\sum_{j=0}^{m}{a_j f(t-j\Delta t)}
\end{align*}
can be modfied with an auto-regressive component to be an autoregressive model with exogenous inputs (ARX) such as that used in \cite{ARXEqn}, taking the form:
\begin{align}
\hat{x}(t+\Delta t)&=\sum_{i=0}^la_i\cdot x(t-i\Delta t)+\sum_{j=0}^m b_j\cdot f(t-j\Delta t)+c
\label{ARXEqn}
\end{align}
Where $m$ and $l$ are the number of coefficients desired for including previous data points in the prediction, and $c$ is a factor to remove the mean offset from the data. Note that in some cases the starting value of the iterators can be individually increased if there is a known delay in response time or there is a desire to predict further into the future. In \cite{ARXEqn}, second order equations ($m=2$) were used with anywhere from one to four driving coefficients, but in practice any number of coefficients and any number of driving variables can be used up to some fraction of the number of data points that allows the coefficient matrices to be solved for. 

There generally is a limit to the usefulness of large-lag data \cite{ExtremeEvents}. By looking at a plot of the cross correlation relative to the number of coefficients, a limit will generally be seen where adding more coefficients no longer reduces error in the model. By creating a threshhold of change in fit per coefficient added (perhaps via a bootstrap method), the minimum number of coefficients needed to optimally model the system can be determined.

By constructing a linear system of equations from Equation \ref{ARXEqn}, the coefficients can be solved for in a general matrix form (where, in this case, $l=m$):
\[
\left( \begin{array}{ccccccc}
x_0 & ... & x_{l-1} & f_0 & ... & f_{l-1} & 1\\
x_1 &     & x_l & f_l &  &f_l & 1\\
... &     &     &     &  &   & \\
x_{N-l} & ... & x_{N-1} & f_{N-l} & ... & f_{N-1} & 1
\end{array} \right)
\left(\begin{array}{c}
a_0\\...\\a_{l-1}\\b_0\\...\\b_{l-1}\\c
\end{array}\right)
=
\left(
\begin{array}{c}
x_l \\ x_{l+1} \\ ... \\ x_{N}
\end{array}
\right)
\]

This is a linear model for the behavior of a system. However, it has been shown that the set of coefficients describing the response of a system can change with storm intensity \cite{ARXEqn}, the time scale modeled \cite{Coupling}, and even the time of day \cite{VBzAL}. This creates a very large number of possible directions for research, from predicting storm onsets, to predicting storm intensities, to modeling the overall shape and behavior of a storm, as well as all of the other possible interactions outside of storm-time. 

\subsection{Caveats}
There will be a number of things that, ideally, must come together to make this kind of data prediction work. For one: ARX methods can often be heavily dependent on a concept known as "persistence", whereby the best prediction for a variable at any time is that same variable at the last measured time step. For example, if the high temperature today is $70^\circ$, it is fairly likely that the high temperature tomorrow will be near $70^\circ$. Too much reliance on persistence forecasting, though, and predictions can lose their usefulness. Figure \ref{persist}, for example, shows how a model can achieve high correlation with persistence, but be almost entirely useless for predicting events before they happen. \inote{Bit more detail why}

\begin{figure}[h!]
\centering
\includegraphics[scale=0.45]{{Figures/GICshift.png}}
\includegraphics[scale=0.45]{{Figures/GICCoef.png}}
\caption{Persistence forecast; model in red}
\label{persist}
\end{figure}

In this case, it is clear that the largest auto-correlation coefficient comes at 1 time lag, meaning the most recent measurement has the most weight in a forecast. For day-to-day behavior, this is acceptable as being part of the behavior of the system. For the forecasting of extreme events, however, another metric must be used that measures the ability of the model to predict events at or before their actual occurrence, while simultaneously avoiding predicting events that do not happen. One method for comparing models in this fashion is by using the Heidke Skill Score \cite{Heidke,Brier}, which is based on the quantity:
\begin{align*}
S=\frac{R-E}{T-E}
\end{align*}
where $R$ is the number of correct forecasts, $T$ is the total number of forecasts, and $E$ is the number expected to be correct by, in this case, a persistence forecast. This can be adapted to either consider a range of "correctness", or a binary threshold to be met. It may also be desired to assign a cost-weighting to success rates. If, say, it costs \$1 million to prepare a power grid for a storm, 10 false alarms to every one storm gets costly unless successfully preparing for that one storm saves \$1 billion. To do this, a measure of the utility of a forecast can be quantified \cite{WeigelDecision}:
\begin{align*}
U_F\equiv BN_H-CN_{\bar{H}}>0
\end{align*}
Where $N_H$ is the number of correct forecasts, $N_{\bar{H}}$ is the number of false alarms, $C$ is the cost of taking mitigating action, and $B$ is the benefit from correctly taking mitigating action. This method has caveats discussed in \cite{WeigelDecision}, but is a useful metric for forecast utility when costs are known, and some measure of success can be determined. 

The other major problem in forecasting is that of lead time. Being able to forecast a storm one minute in advance is generally not enough time for operators to take mitigating action.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.50]{{Figures/GIClags.png}}
\caption{Correlation vs lags}
\label{Lags}
\end{figure}

Figure \ref{Lags} shows a set of predictions made for autocorrelation in geomagnetically-induced currents (GIC). This metric is a measure of how much electric currents in the magnetosphere induce currents in ground-based electrical systems. The predictions were made further and further in time from the current magnetic field and GIC measurements, with decreasing accuracy as the predicted time got further from the current time. While an accurate prediction can be made one minute in advance, a prediction 3 hours in advance has almost no correlation with what actually happens. This is the main problem that this dissertation hopes to address.

\section{ARMAX}

\subsection{Overview}
A class of model known as an Auto-Regressive Moving Average with eXogenous inputs model (ARMAX) is often used in time series analysis to combine the effects of persistence, linear dependence, and an average that changes with time. It makes a slight change on the ARX model in Equation \ref{ARXEqn}, adding the moving average term:
\begin{align}
\hat{x}(t+\Delta t)&=\sum_{i=0}^la_i\cdot x(t-i\Delta t)+\sum_{j=0}^m b_j\cdot f(t-j\Delta t)+\sum_{k=0}^n c_k\cdot g(t-k\Delta t)+c_{t+\Delta t}
\label{ARMAXEqn}
\end{align}
\note Clean up notation


\subsection{Applicability}
As implied by the name, an ARMAX model is suitable for analysis of a time-dependent linear system where the value of a measurement is determined by its own persistence, an external variable, and some factor that contributes to a moving average with time. Most linear systems can be encapsulated by this framework, some even being overspecified with this level of accounting for variability.

\subsection{Caveats and Biases}
Though by the nature of least squares analysis used for linear models, an optimal (minimal error) solution will be found, there is still room for overfitting due to noise, or for seemingly fitting a linear solution to a nonlinear system. If you allow for enough degrees of freedom, any system can be fit by a linear model. \inote{Explain or cite}

\subsection{Mean vs Median}
The question of whether to use means or medians for analysis is based on what facets of the data are most important to the research. Since means biased towards outliers and medians biased against them, the decision rests on how much weight should be given to outliers (or extreme values) in a study. For example, when looking at long-term solar wind variables, intermittent spikes may not be relevant to the overall pattern of behavior being analyzed, but knowing that on a short time scale a certain day had a noticeable spike may be important. In the former case, using the median would likely be best, and using the mean for the latter. \inote{Too colloquial?}

\subsection{Effects of time averaging}
Similarly to the mean vs median question, the decision on if/how much to average the data over time will affect the resulting time series to be biased against intermittent spikes in value. The more time added to any particular average, the less impact any short-term changes will have on the final value.

\subsection{Results and Analysis}

\section{Nonlinear}

\subsection{Overview}

Other models will be devised to test nonlinear approaches to forecasting. One common choice are models based on neural networks \cite{NNARMA,ANNforecast} which allow for a model that can approximate a non-linear system. The usefulness of this is apparent in a few key points: the weights of contribution of any particular variable to a system will likely be nonlinear in some fashion (e.g. a ground station's measurements will depend on sunlight heating the ionosphere which depends on latitude, time of year, and time of day), and allowing for the non-linear effects of saturation where perhaps the magnetosphere will behave differently after reaching certain levels of particle density or electric potential.

Another algorithm known as Principal Component Analysis (PCA) can be used to take the large number of possible variables and define an orthogonal set of vectors that most efficiently encapsulate the variance in the data. By doing this, the number of variables needed for computing any linear or non-linear algorithm can be reduced and optimized, making predictions quicker while maintaining most of the predictive benefits of using all possible data.

\subsection{Neural Networks}

\subsubsection{Applicability}

\subsubsection{Caveats and Biases}

\subsubsection{Parameters and Inputs}

\subsubsection{Results and Analysis}

\subsubsection{Comparison to Linear Model}

\note Note that you always compare performance relative to linear model as a reference point.  Useful for determining how important nonlinearity is.

